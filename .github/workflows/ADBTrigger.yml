name: Run a notebook within its repo on PRs

on:
  push:
    branches:
      - main
      
env:
  DATABRICKS_HOST: https://adb-7871370843959004.4.azuredatabricks.net/

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checks out the repo
        uses: actions/checkout@v2
      - name: Log into Azure
        uses: Azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      # The step below does the following:
      # 1. Sends a POST request to generate an Azure Active Directory token for an Azure service principal
      # 2. Parses the token from the request response and then saves that in as DATABRICKS_TOKEN in the
      # GitHub enviornment.
      # Note: if the API request fails, the request response json will not have an "access_token" field and
      # the DATABRICKS_TOKEN env variable will be empty.
      - name: Generate and save AAD token
        id: generate-token
        run: |
          echo "DATABRICKS_TOKEN=$(az account get-access-token \
          --resource=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
          --query accessToken -o tsv)" >> $GITHUB_ENV
      - name: Setup Python
        uses: actions/setup-python@v4.5.0
        with:
          python-version: '3.x'
      - name: install python packages
        run: |
          pip install setuptools
          pip install wheel
      - name: Build wheel
        run:
          python ADB/annual-enterprise-survey-2021-financial-year-provisional-csv.csv bdist_wheel
      - name: Upload Wheel
        uses: databricks/upload-dbfs-temp@v0
        id: upload_wheel
        with:
          local-path: dist/my-project.whl    
      - name: Trigger model training notebook
        uses: databricks/run-notebook@v0
        with:
          local-notebook-path: ./ADB/DataRead.py
          libraries-json: >
            [
              { "whl": "${{ steps.upload_wheel.outputs.dbfs-file-path }}" }
            ]
          # If the current workflow is triggered from a PR,
          # run notebook code from the PR's head commit, otherwise use github.sha.
          git-commit: ${{ github.event.pull_request.head.sha || github.sha }}
          # The cluster JSON below is for Azure Databricks. On AWS and GCP, set
          new-cluster-json: >
            {
              "num_workers": 1,
              "spark_version": "10.4.x-scala2.12",
              "node_type_id": "Standard_D3_v2"
            }
          # Grant all users view permission on the notebook results
          access-control-list-json: >
            [
              {
                "group_name": "users",
                "permission_level": "CAN_VIEW"
              }
            ]
