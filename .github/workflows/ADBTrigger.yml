name: Run a notebook within its repo on PRs

on:
  push:
    branches:
      - main
      
env:
  DATABRICKS_HOST: https://adb-7871370843959004.4.azuredatabricks.net/

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checks out the repo
        uses: actions/checkout@v2
      - name: Log into Azure
        uses: Azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      # The step below does the following:
      # 1. Sends a POST request to generate an Azure Active Directory token for an Azure service principal
      # 2. Parses the token from the request response and then saves that in as DATABRICKS_TOKEN in the
      # GitHub enviornment.
      # Note: if the API request fails, the request response json will not have an "access_token" field and
      # the DATABRICKS_TOKEN env variable will be empty.
      - name: Generate and save AAD token
        id: generate-token
        run: |
          echo "DATABRICKS_TOKEN=$(az account get-access-token \
          --resource=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d \
          --query accessToken -o tsv)" >> $GITHUB_ENV
          
      - name: Trigger model training notebook
        uses: databricks/run-notebook@v0
        with:
          local-notebook-path: ./ADB/DataRead.py
          # If the current workflow is triggered from a PR,
          # run notebook code from the PR's head commit, otherwise use github.sha.
          git-commit: ${{ github.event.pull_request.head.sha || github.sha }}
          # The cluster JSON below is for Azure Databricks. On AWS and GCP, set
          new-cluster-json: >
            {
              "num_workers": 0,
              "cluster_name": "Rajat Rath's Cluster",
              "spark_version": "11.3.x-scala2.12",
              "spark_conf": {
                  "spark.databricks.cluster.profile": "singleNode",
                  "spark.master": "local[*, 4]",
                  "spark.databricks.delta.preview.enabled": "true"
              },
              "azure_attributes": {
                  "first_on_demand": 1,
                  "availability": "ON_DEMAND_AZURE",
                  "spot_bid_max_price": -1
              },
              "node_type_id": "Standard_DS3_v2",
              "driver_node_type_id": "Standard_DS3_v2",
              "ssh_public_keys": [],
              "custom_tags": {
                  "ResourceClass": "SingleNode"
              },
              "spark_env_vars": {
                  "PYSPARK_PYTHON": "/databricks/python3/bin/python3"
              }
            }
          # Grant all users view permission on the notebook results
          access-control-list-json: >
            [
              {
                "group_name": "users",
                "permission_level": "CAN_VIEW"
              }
            ]
